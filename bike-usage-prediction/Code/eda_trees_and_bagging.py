# -*- coding: utf-8 -*-
"""eda_trees_and_bagging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-db_K7CVLyGVuzhyYD364FTVpjJdqeV8
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math as mt
import seaborn as sns

from google.colab import drive
drive.mount('/content/gdrive')


data = pd.read_csv('/content/gdrive/MyDrive/SML_project/training_data_fall2024.csv')
data.head()


data.groupby('increase_stock').size().plot(kind='barh',
                                      color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)


stock_counts = data.groupby(['hour_of_day',
                             'increase_stock']).size().unstack(fill_value=0)
plt.figure(figsize=(12, 6))
stock_counts.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='viridis',
                  edgecolor='black')
plt.title('Stock Increase Categories by Hour of Day', fontsize=16)
plt.xlabel('Hour of Day', fontsize=14)
plt.ylabel('Count of Increase Stock', fontsize=14)
plt.xticks(rotation=0)
plt.legend(title="Increase Stock Category", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


stock_counts = data.groupby(['month',
                             'increase_stock']).size().unstack(fill_value=0)
plt.figure(figsize=(12, 6))
stock_counts.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='viridis',
                  edgecolor='black')
plt.title('Stock Increase Categories by Month', fontsize=16)
plt.xlabel('Month', fontsize=14)
plt.ylabel('Count of Increase Stock', fontsize=14)
plt.xticks(rotation=0)
plt.legend(title="Increase Stock Category", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()


stock_counts = data.groupby(['day_of_week',
                             'increase_stock']).size().unstack(fill_value=0)
plt.figure(figsize=(12, 6))
stock_counts.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='viridis',
                  edgecolor='black')
plt.title('Stock Increase Categories by Day of Week', fontsize=16)
plt.xlabel('Day of Week', fontsize=14)
plt.ylabel('Count of Increase Stock', fontsize=14)
plt.xticks(rotation=0)
plt.legend(title="Increase Stock Category", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()



# Group and count data
stock_counts = data.groupby(['weekday',
                             'increase_stock']).size().unstack(fill_value=0)

# Plotting
plt.figure(figsize=(12, 6))
ax = stock_counts.plot(kind='bar', stacked=True, figsize=(12, 6),
                       colormap='viridis', edgecolor='black')

# Adding title and labels
plt.title('Stock Increase Categories by Weekday', fontsize=16)
plt.xlabel('Weekday', fontsize=14)
plt.ylabel('Count of Increase Stock', fontsize=14)
plt.xticks(rotation=0)
plt.legend(title="Increase Stock Category", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Adding ratio annotations
for container in ax.containers:
    for bar in container:
        # Get the height of the bar (individual count)
        height = bar.get_height()
        if height > 0:  # Only annotate non-zero bars
            # Get the x position and total for the group
            total = stock_counts.sum(axis=1)[int(bar.get_x() + 0.5)]
            # Calculate the ratio
            ratio = height / total
            # Annotate the bar with the ratio
            ax.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() / 2,
                f'{ratio:.2%}',  # Format as a percentage
                ha='center',
                va='center',
                fontsize=10,
                color='white'
            )

# Show the plot
plt.show()


# Group and count data
stock_counts = data.groupby(['holiday',
                             'increase_stock']).size().unstack(fill_value=0)

# Plotting
plt.figure(figsize=(12, 6))
ax = stock_counts.plot(kind='bar', stacked=True, figsize=(12, 6),
                       colormap='viridis', edgecolor='black')

# Adding title and labels
plt.title('Stock Increase Categories by Holiday', fontsize=16)
plt.xlabel('Holiday', fontsize=14)
plt.ylabel('Count of Increase Stock', fontsize=14)
plt.xticks(rotation=0)
plt.legend(title="Increase Stock Category", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

# Adding ratio annotations
for container in ax.containers:
    for bar in container:
        # Get the height of the bar (individual count)
        height = bar.get_height()
        if height > 0:  # Only annotate non-zero bars
            # Get the x position and total for the group
            total = stock_counts.sum(axis=1)[int(bar.get_x() + 0.5)]
            # Calculate the ratio
            ratio = height / total
            # Annotate the bar with the ratio
            ax.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() / 2,
                f'{ratio:.2%}',  # Format as a percentage
                ha='center',
                va='center',
                fontsize=10,
                color='white'
            )

# Show the plot
plt.show()


ddata = data.drop(['snow', 'summertime'], axis = 1)
ddata.describe()


grouped_data = data.groupby(['month'])[['temp', 'precip', 'snowdepth',
                                        'humidity', 'dew' ,'windspeed',
                                        'cloudcover', 'visibility']].mean()
print(grouped_data)


data.dtypes


for col in data.columns:
  print(col, len(data[col].unique()))


#data_with_dummies = pd.get_dummies(ddata, columns=['increase_stock'], prefix='stock', drop_first=False)
#data_with_dummies = data_with_dummies.drop(['increase_stock'], axis = 1)
ddata['increase_stock'] = ddata['increase_stock'].replace({
    'low_bike_demand': 0,
    'high_bike_demand': 1
})
ddata.head()


#Correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(ddata[['holiday', 'weekday', 'temp', 'dew', 'humidity', 'precip',
                   'snowdepth', 'windspeed', 'cloudcover', 'visibility',
                   'increase_stock']].corr(), annot=True, cmap='coolwarm',
                    fmt=".2f")

plt.title('Correlation Matrix', fontsize=16)
plt.show()


holiday_data = ddata[(ddata['holiday'] == 1)]
non_holiday_data = ddata[(ddata['holiday'] == 0)]
ddata.head()
print(len(holiday_data[(holiday_data['increase_stock'] == 0)])/len(
    holiday_data[(holiday_data['increase_stock'] == 1)]))
print(len(non_holiday_data[(non_holiday_data['increase_stock'] == 0)])/len(
    non_holiday_data[(non_holiday_data['increase_stock'] == 1)]))


(ddata['weekday'] - ddata['holiday']).sum()


ddata.describe()


(ddata['precip'] > 0)


data['rainy_day'] = (data['precip'] > 0).astype(int)
data['snowy_day'] = (data['snowdepth'] > 0).astype(int)



# Group and count data
stock_counts = data.groupby(['snowy_day',
                             'increase_stock']).size().unstack(fill_value=0)

# Plotting
plt.figure(figsize=(12, 6))
ax = stock_counts.plot(kind='bar', stacked=True, figsize=(12, 6),
                       colormap='viridis', edgecolor='black')

# Adding title and labels
plt.title('Stock Increase Categories by Snowy Day', fontsize=16)
plt.xlabel('Snowy Day', fontsize=14)
plt.ylabel('Count of Increase Stock', fontsize=14)
plt.xticks(rotation=0)
plt.legend(title="Increase Stock Category", fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()

for container in ax.containers:
    for bar in container:
        height = bar.get_height()
        if height > 0:  # Only annotate non-zero bars
            # Get the x position and total for the group
            total = stock_counts.sum(axis=1)[int(bar.get_x() + 0.5)]
            ratio = height / total
            print (ratio)
            # Annotate the bar with the ratio
            ax.text(
                bar.get_x() + bar.get_width() / 2,
                bar.get_height() / 2,
                f'{ratio:.2%}',  # Format as a percentage
                ha='center',
                va='center',
                fontsize=10,
                color='white'
            )

# Show the plot
plt.show()


data=data.replace(to_replace="low_bike_demand",value=0)
data=data.replace(to_replace="high_bike_demand",value=1)
# data.head()
precip_data = data[(data['snowdepth'] > 0)]
non_precip_data = data[(data['snowdepth'] == 0)]
data.head()
print(len(precip_data[(precip_data['increase_stock'] == 0)])/len(
    precip_data))
print(len(non_precip_data[(non_precip_data['increase_stock'] == 0)])/len(
    non_precip_data))


from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score


data['increase_stock'] = data['increase_stock'].replace({
   'low_bike_demand': 0,
   'high_bike_demand': 1
   })


X = data.drop(columns=['increase_stock', 'summertime', 'snow'])
y = data['increase_stock']

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1,
                                                            random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,
                                              test_size=0.111, random_state=42)


tree_model = DecisionTreeClassifier(random_state=42)
tree_model.fit(X_train, y_train)

y_pred = tree_model.predict(X_val)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


best_model = None
best_f1_score = 0

for depth in [3, 5, 10, None]:
    for min_samples_split in [2, 5, 10]:
        for min_samples_leaf in [1, 2, 5]:
            tree_model = DecisionTreeClassifier(
                max_depth=depth,
                min_samples_split=min_samples_split,
                min_samples_leaf=min_samples_leaf,
                random_state=42
            )
            tree_model.fit(X_train, y_train)
            y_pred = tree_model.predict(X_val)
            # F1_score for 'high_bike_demand'
            f1_class_1 = f1_score(y_val, y_pred, average='weighted')
            score = accuracy_score(y_val, y_pred)

            if f1_class_1 > best_f1_score:
                best_f1_score = f1_class_1
                best_score = score
                best_model = tree_model
                best_params = {
                    'max_depth': depth,
                    'min_samples_split': min_samples_split,
                    'min_samples_leaf': min_samples_leaf
                }

print("Best score:", best_score)
print("Best F1 score:", best_f1_score)
print("Best Parameters:", best_params)


from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'criterion': ['gini', 'entropy']
}

grid_search = GridSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,  # 5-fold cross-validation
    n_jobs=-1,  # Use all available processors
    verbose=1
)

grid_search.fit(X_train, y_train)
y_pred = grid_search.predict(X_val)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


y_pred = grid_search.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))


data.head()


# Sunrise and Sunset times (rounded to nearest hour)
sunrise_sunset = {
    1: {'sunrise': 7, 'sunset': 17},
    2: {'sunrise': 7, 'sunset': 18},
    3: {'sunrise': 7, 'sunset': 19},
    4: {'sunrise': 6, 'sunset': 20},
    5: {'sunrise': 5, 'sunset': 20},
    6: {'sunrise': 5, 'sunset': 21},
    7: {'sunrise': 6, 'sunset': 21},
    8: {'sunrise': 6, 'sunset': 20},
    9: {'sunrise': 6, 'sunset': 19},
    10: {'sunrise': 7, 'sunset': 18},
    11: {'sunrise': 7, 'sunset': 17},
    12: {'sunrise': 7, 'sunset': 17}
}


# Function to determine if it's day based on hour_of_day
def determine_day(row):
    sunrise = sunrise_sunset[row['month']]['sunrise']
    sunset = sunrise_sunset[row['month']]['sunset']
    if sunrise <= row['hour_of_day'] < sunset:
        return 1
    else:
        return 0


data['day'] = data.apply(determine_day, axis=1)


mean_humidity = data['humidity'].mean()
mean_temp = data['temp'].mean()


data['good_weather_humid_temp'] = 0  # Initialize the column with 0s
data.loc[(data['humidity'] < mean_humidity) & (data['temp'] < mean_temp),
         'good_weather_humid_temp'] = 1


data['rainy_day'] = (data['precip'] > 0).astype(int)
data['snowy_day'] = (data['snowdepth'] > 0).astype(int)
data['good_weather_rain_snow'] = 0
data.loc[(data['rainy_day'] == 0) & (data['snowy_day'] == 0),
         'good_weather_rain_snow'] = 1


X = data.drop(columns=['increase_stock', 'summertime', 'snow'])
y = data['increase_stock']

# Stratify maintains the same class distribution in both train and test
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1,
                                                            random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,
                                              test_size=0.111, random_state=42)


tree_model = DecisionTreeClassifier(random_state=42)
tree_model.fit(X_train, y_train)

y_pred = tree_model.predict(X_val)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 5],
    'criterion': ['gini', 'entropy']
}

grid_search = GridSearchCV(
    estimator=DecisionTreeClassifier(random_state=42),
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,  # 5-fold cross-validation
    n_jobs=-1,  # Use all available processors
    verbose=1
)

grid_search.fit(X_train, y_train)
y_pred = grid_search.predict(X_val)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


y_pred = grid_search.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))


importances = tree_model.feature_importances_
feature_names = X_train.columns
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

feature_importance_df = feature_importance_df.sort_values(by='Importance',
                                                          ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'],
         color='skyblue')
plt.xlabel('Importance')
plt.title('Feature Importance - Decision Tree')
plt.tight_layout()
plt.show()



prime_features = ['hour_of_day', 'temp', 'humidity', 'windspeed', 'dew',
                  'cloudcover', 'month', 'day_of_week', 'precip', 'visibility',
                  'holiday']

X = data[prime_features]
y = data['increase_stock']

# Stratify maintains the same class distribution in both train and test

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1,
                                                            random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,
                                              test_size=0.111, random_state=42)

tree_model = DecisionTreeClassifier(random_state=42)
tree_model.fit(X_train, y_train)

y_pred = tree_model.predict(X_val)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


y_pred = tree_model.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))


# prime_features = ['hour_of_day', 'temp', 'humidity', 'windspeed', 'dew',
# 'cloudcover', 'month', 'day_of_week', 'precip', 'visibility', 'holiday']

X = data.drop(columns=['increase_stock', 'summertime', 'snow'])
y = data['increase_stock']

# Stratify maintains the same class distribution in both train and test

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1,
                                                            random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,
                                              test_size=0.111, random_state=42)

rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_val)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


y_pred = rf_model.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))


param_grid = {
    'n_estimators': [10, 50, 100, 200],  # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],  # Maximum depth of each tree
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node
    'min_samples_leaf': [1, 2, 4],    # Minimum number of samples required to be at a leaf node
    'bootstrap': [True, False]        # Whether to use bootstrap samples when building trees
}

grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,  # 5-fold cross-validation
    n_jobs=-1,  # Use all available processors
    verbose=1
)

grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

y_pred = grid_search.predict(X_val)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


y_pred = grid_search.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))


importances = rf_model.feature_importances_
feature_names = X_train.columns
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
})

feature_importance_df = feature_importance_df.sort_values(by='Importance',
                                                          ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'],
         color='skyblue')
plt.xlabel('Importance')
plt.title('Feature Importance - Decision Tree')
plt.tight_layout()
plt.show()


X = data.drop(columns=['increase_stock', 'summertime', 'snow'])
y = data['increase_stock']

# Stratify maintains the same class distribution in both train and test
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1,
                                                            random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val,
                                              test_size=0.111, random_state=42)

bag_model = BaggingClassifier(random_state=42)
bag_model.fit(X_train, y_train)

y_pred = bag_model.predict(X_val)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


y_pred = bag_model.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))


param_grid = {
    'n_estimators': [10, 50, 100, 200],  # Number of base estimators (trees)
    'max_samples': [0.5, 0.8, 1.0],   # Proportion of samples to train each base estimator on
    'max_features': [0.5, 0.8, 1.0],  # Proportion of features to train each base estimator on
    'bootstrap': [True, False],       # Whether to use bootstrap samples
    'n_jobs': [-1]                    # Use all available processors
}

grid_search = GridSearchCV(
    estimator=BaggingClassifier(random_state=42),
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,  # 5-fold cross-validation
    n_jobs=-1,  # Use all available processors
    verbose=1
)

grid_search.fit(X_train, y_train)

print("Best Parameters:", grid_search.best_params_)
print("Best Score:", grid_search.best_score_)

y_pred = grid_search.predict(X_val)

print("Classification Report:\n", classification_report(y_val, y_pred))
print("Accuracy Score:", accuracy_score(y_val, y_pred))


y_pred = grid_search.predict(X_test)

print("Classification Report:\n", classification_report(y_test, y_pred))
print("Test Accuracy:", accuracy_score(y_test, y_pred))